

# 数据仓库之ETL实战

ETL，Extraction-Transformation-Loading的缩写，中文名称为数据抽取、转换和加载。一般随着业务的发展扩张，产线也越来越多，产生的数据也越来越多，这些数据的收集方式、原始数据格式、数据量、存储要求、使用场景等方面有很大的差异。作为数据中心，既要保证数据的准确性，存储的安全性，后续的扩展性，以及数据分析的时效性，这是一个很大的挑战。


名词解释：

ODS——操作性数据
DW——数据仓库
DM——数据集市

![img](../imgs/database/数据中心整体架构.png)

![img](/Users/leinian/note/imgs/database/ETL概念.png)

### 一、数据抽取

数据抽取是指把ODS源数据抽取到DW中，然后处理成展示给相关人员查看的数据

源数据：

- 用户访问日志

- 自定义事件日志、操作日志
- 业务日志
- 各服务产生的日志
- 系统日志：操作系统日志，CDN日志等
- 监控日志
- 其它日志

抽取频次：

- 如果没有特殊要求可以一天一次，但是需要避开拉去日志的高峰期
- 对于有实时性要求的日志，可以一小时一次，或者直接使用kafka等相关工具收集，需要考虑到系统能否承受

抽取策略：

- 由于数据量较大，一般都是采用增量抽取，但是对于一些特殊场景的数据，比如订单数据，由于订单的状态会发生变化，并且订单的量级是可预知和相对较少的，就需要采用全量拉取的策略*

- 对于增量拉取的日志，如果是文件类型，可以在文件名称上追加日期，例如 server_log_2018082718.log，这样就可以满足按小时拉取的需求

- 对于源数据的保留，考虑到突发情况，服务器上的源数据至少要保证2天以上的时间

### 二、数据转换、清洗

顾名思义，就是把不需要的和不符合规范的数据进行处理。数据清洗最好不要放在抽取的环节进行，考虑到有时可能会查原始数据。一般各公司都会有自己的规范，以下列出几点仅供参考

数据清洗主要包括以下几个方面：

1. 空值处理；根据业务需要，可以将空值替换为特定的值或者直接过滤掉;
2. 验证数据正确性；主要是把不符合业务含义的数据做一处理，比如，把一个表示数量的字段中的字符串替换为0，把一个日期字段的非日期字符串过滤掉等等；
3. 规范数据格式；比如，把所有的日期都格式化成yyyy-MM-dd HH:mm:ss的格式等；
4. 数据转码；把一个源数据中用编码表示的字段，通过关联编码表，转换成代表其真实意义的值等等；
5. 数据标准，统一；比如在源数据中表示男女的方式有很多种，在抽取的时候，直接根据模型中定义的值做转化，统一表示男女；
6. 其他业务规则定义的数据清洗...

### 三、数据加载

数据拉取，清洗完之后，就需要展示了。一般是把清洗好的数据加载到mysql中，然后在各系统中使用，或者使用Tableau直接给相关人员展示

### 四、ETL相关工具

ELT相关的工具有很多，这里只列举一些常用的，而且各公司的技术原型也不一样，就需要根据实际情况来选择

数据抽取工具：

kafka

flume

sync

数据清洗

hive/tez
pig/tez
storm
spark

其它工具

数据存储：hadoop、hbase，ES、redis
任务管理：azkaban、oozie
数据同步：datax、sqoop

### 五、ETL过程中的元数据

试想一下，你作为一个新人接手别人的工作，没有文档，程序没有注释，数据库中的表和字段也没有任何comment，你是不是会望着窗外，一声长叹...所以元数据管理系统对于数据仓库来说是必须的，并且相关人员必须定时维护，如果元数据和数据仓库中的变动不同步，那么元数据系统就形同虚设。这里说一句：对于元数据管理不应该是规范，应该是硬性规定。